{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "*by [GRAM](https://gra.m-gan.sl), Head of Data Science, Expero Inc*\n",
    "\n",
    "There's no magic here. \n",
    "\n",
    "We've all heard a proselytizing hyperbolist make the an-AI-is-going-to-steal-my-job speech. If you subscribe, look at the following code cell. That cell demonstrates all the tenets of deep learning's building blocks.\n",
    "\n",
    "A neural network is nothing but a nonlinear system of equations like $y = \\sigma(w x + b)$. Seriously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm, tnrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "def sigmoid(x, forward=True):\n",
    "    if forward:\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    else: # this is the derivative of sigmoid (for backprop)\n",
    "        return x * (1 - x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_forward(inpt, weight1, weight2):\n",
    "    layer1 = np.matmul(inpt, weight1) + bias1 # layer 1 matrix multiplication and addition\n",
    "    layer1_sigmoid = sigmoid(layer1) # layer 1 nonlinearity\n",
    "\n",
    "    layer2 = np.matmul(layer1_sigmoid, weight2) + bias2 # layer 2 matrix multiplication and addition\n",
    "    \n",
    "    return layer2, layer1_sigmoid, layer1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those interested, the above cell is a nested set of nonlinear equations. It looks like this:\n",
    "\n",
    "\\begin{equation*}\n",
    "y = \\sigma(x * w_1 + b_1) * w_2 + b_2,\n",
    "\\end{equation*}\n",
    "\n",
    "where $x$ is the input, $w_1, w_2, b_1 , b_2$ are weight matrices and bias vectors for layers 1 and 2 respectively, and $\\sigma()$ is a nonlinear function (the \"sigmoid\" function in the rest of this notebook).\n",
    "\n",
    "\n",
    "For those uninterested, check out this picture of the above neural network with parameters we'll define in subsequent code blocks:\n",
    "\n",
    "![image](nn_arch.png)\n",
    "\n",
    "Here we see a simple neural network which takes one number as input (the green layer) and outputs one number (the red layer). In the middle (the orange layer) we have a so-called \"hidden layer,\" which in this case is a 300 number array. Moving information from input layer to hidden layer to output layer is as simple as matrix multiplying and adding numbers. In the middle we apply a sigmoid function to each of the numbers.\n",
    "\n",
    "**We care about this because we can \"teach\" this simple system to act as a model between one set of numbers and another set.** For example, we can train this system to output a 1 when we input a 1, a 2 when we input a 2, and an N when we input an N. This is equivalent to building a linear model, but, more interestingly, we could teach it to output a nonlinear model: 1 maps to 1, 2 maps to 4, and N maps to $N^2$. In the following code, we'll train this model to learn a sine function.\n",
    "\n",
    "How does it \"learn?\" By adjusting the \"w\"s and \"b\"s you see in the equations above. The mechanics here are important, so if you have mathematical proclivities pick through the following code block. The *TL;DR* is that we show the system a bunch of corresponding input/output pairs we want it to learn, and we show it these pairs thousands of times. Every time we show it these pairs, we subtract a little bit off of each of the \"w\"s and \"b\"s in the direction that makes the outputs of the network more similar to the known output we're trying to teach it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neural_network_backward(inpt, target,\n",
    "                            layer1, layer1_sigmoid, layer2,\n",
    "                            weight1, weight2, bias1, bias2,\n",
    "                            learning_rate):\n",
    "\n",
    "    #delta_output = np.zeros(layer2.shape[0]) # output layer error signal\n",
    "    #delta_hidden = np.zeros(layer1.shape[0]) # hidden layer error signal\n",
    "\n",
    "    weight1gradient = np.zeros(weight1.shape) # weight 1 gradient\n",
    "    weight2gradient = np.zeros(weight2.shape) # weight 2 gradient\n",
    "    \n",
    "    #bias1gradient = np.zeros(bias1.shape) # bias 1 gradient\n",
    "    #bias2gradient = np.zeros(bias2.shape) # bias 2 gradient\n",
    "\n",
    "    delta_output = (layer2 - target) # calculate error signal for output layer\n",
    "    weight2gradient[:,0] = delta_output * layer1_sigmoid # update weight gradient for output layer\n",
    "    weight2 -= learning_rate * weight2gradient # update weights for output layer\n",
    "\n",
    "    bias2gradient = delta_output # bias gradient for output layer\n",
    "    bias2gradient -= learning_rate * bias2gradient # update biases for output layer\n",
    "    \n",
    "    delta_hidden = sigmoid(layer1_sigmoid, forward=False) * delta_output * weight2[:,0] # calculate error signal for hidden layer\n",
    "    weight1gradient[0,:] = delta_hidden * inpt # update gradient for hidden layer\n",
    "    weight1 -= learning_rate * weight1gradient # update weights for hidden layer\n",
    "    \n",
    "    bias1gradient = delta_hidden # bias gradient for hidden layer\n",
    "    bias1gradient -= learning_rate * bias1gradient # update biases for hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code block is called optimization by backpropagation of gradients. It's a critical piece of thinking which has enabled the deep learning revolution all industries are undergoing.\n",
    "\n",
    "To demonstrate this optimization step, let's try to get the above neural network to learn a sine function, defined next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate some data\n",
    "x = np.linspace(start=0, stop=2 * np.pi, num=200)\n",
    "y = np.sin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "# instantiate weights and biases\n",
    "np.random.seed(1)\n",
    "\n",
    "weight1 = np.random.uniform(low=-1, high=1, size=(1,300))\n",
    "weight2 = np.random.uniform(low=-1, high=1, size=(300,1))\n",
    "\n",
    "bias1 = np.random.uniform(low=-1, high=1, size=300)\n",
    "bias2 = np.random.uniform(low=-1, high=1, size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true,
    "tags": [
     "hide"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "# pre train plotting\n",
    "pre_train = []\n",
    "for i in range(x.shape[0]):\n",
    "    pre_train.append(neural_network_forward([x[i]], weight1, weight2)[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One at a time, we expose the network to the input/output pairs (these are labeled x and y respectively in the code below). According to our diagram above, the input goes into the green slot and we adjust the orange neurons to make the red slot from the network a tiny bit closer (0.1%) to the true output of the sine function.\n",
    "\n",
    "We do this 20,000 times. Every time we do, we output the \"mean squared error\" (half of the squared distance) between the network's prediction and the groundtruth output from the sine function. After these twenty thousand iterations, or \"epochs,\" we draw out a plot of that loss at each step. If the network is learning anything, we expect the loss to decrease, as the predictions are getting closer to the groundtruth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84fdd1ef2b35442fbe20da17cff5d4d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training\n",
    "num_epochs = 20000\n",
    "learning_rate = 0.001\n",
    "loss_history = []\n",
    "\n",
    "for i in tnrange(num_epochs):\n",
    "    for idx in range(x.shape[0]):\n",
    "        sample = np.array([x[idx]])\n",
    "        target = np.array([y[idx]])\n",
    "\n",
    "        layer2, layer1_sigmoid, layer1 = neural_network_forward(sample, weight1, weight2)\n",
    "\n",
    "        neural_network_backward(sample, target,\n",
    "                                layer1, layer1_sigmoid, layer2,\n",
    "                                weight1, weight2, bias1, bias2,\n",
    "                                learning_rate)\n",
    "\n",
    "    loss_history.append(.5 * np.square(layer2 - target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "# loss plotting\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Training Loss History', fontsize=20)\n",
    "plt.xlabel('Epoch Number')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "\n",
    "plt.plot(loss_history, label='loss')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss decreased dramatically over the course of twenty thousand epochs, so presumably the network has learned something. To test this theory, we'll plot a bunch of network outputs before and after training.\n",
    "\n",
    "Note in the next code block that we're actually feeding the network inputs which it hasn't been trained on. In particular, some of these inputs are outside the \"span of the input domain,\" which simply means we're feeding in values outside the [0, 2$\\pi$] values we trained on. This is an important concept in statistical modeling known as the \"generalizability\" of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate predictions on trained values\n",
    "post_train = []\n",
    "for i in range(x.shape[0]):\n",
    "    post_train.append(neural_network_forward([x[i]], weight1, weight2)[0][0])\n",
    "\n",
    "# calculate predictions on blind values inside and outside of training domain\n",
    "blind = np.arange(-2,8,.5)\n",
    "blind_preds = []\n",
    "for i in range(blind.shape[0]):\n",
    "    blind_preds.append(neural_network_forward([blind[i]], weight1, weight2)[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "tags": [
     "hidein"
    ]
   },
   "outputs": [],
   "source": [
    "#\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.title('Neural Network Evaluation', fontsize=20)\n",
    "plt.xlabel('x (input signal)')\n",
    "plt.ylabel('y (output signal)')\n",
    "\n",
    "plt.plot(x, y, label='groudtruth: y = sin(x)')\n",
    "plt.plot(x, pre_train, label='untrained predictions')\n",
    "plt.plot(x, post_train, label='trained predictions')\n",
    "\n",
    "plt.scatter(blind, blind_preds, color='red', label='interesting predictions')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "#plt.savefig('output.png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The blue line is the true sine function we coded. The orange line is the output from the network before training. The green line is the output from the network after training. The red dots are points of interest in the output from the trained model. So, indeed, our neural network has learned to approximate a sine function!\n",
    "\n",
    "If you're interested in understanding this work more deeply, here are two questions to answer:\n",
    "1. Why doesn't the blue line match the green line perfectly, especially at the extrema of the original function?\n",
    "2. Is it interesting that the red dots turn around and head back in the increasing direction on the left side of the plot?\n",
    "\n",
    "That's it. That's all of deep learning. Multiply, add, apply nonlinearity. No magic. No \"intelligence.\" Just simple arithmetic. Told ya."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
